{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602fbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "spark = SparkSession.Builder().master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1d5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "\n",
    "id_ = StructField('id', LongType())\n",
    "name = StructField('name', StringType())\n",
    "class_ = StructField('class', IntegerType())\n",
    "\n",
    "sex = StructField('sex', StringType())   # 和命名有关\n",
    "age = StructField('age', IntegerType())\n",
    "info = StructField('info', StructType([sex, age]))\n",
    "\n",
    "interest = StructField('interest', ArrayType(StringType()))\n",
    "score = StructField('score', MapType(StringType(), IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4174c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, \"建国\", 1, {'sex': '男', 'age': 17}, ['篮球', '足球'], {'ch': 92, 'en': 75, 'math': 67}],\n",
    "    [2, \"秀兰\", 1, {'sex': '女', 'age': 17}, ['排球', '爬山', '唱歌'], {'ch': 92, 'en': 45}],\n",
    "    [3, \"小妹\", 1, {'sex': '女', 'age': 15}, ['篮球', '足球', '跳舞', 'kpop'], {'ch': 92, 'en': 75, 'math': 47}],\n",
    "    [4, \"翠花\", 1, {'sex': '女', 'age': 14}, ['美术'], {'ch': 92}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaaa9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "| id|name|class|    info|                interest|               score|\n",
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "|  1|建国|    1|{男, 17}|            [篮球, 足球]|{en -> 75, math -...|\n",
      "|  2|秀兰|    1|{女, 17}|      [排球, 爬山, 唱歌]|{ch -> 92, en -> 45}|\n",
      "|  3|小妹|    1|{女, 15}|[篮球, 足球, 跳舞, kpop]|{en -> 75, math -...|\n",
      "|  4|翠花|    1|{女, 14}|                  [美术]|          {ch -> 92}|\n",
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([id_, name, class_, info, interest, score])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c3fc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "| id|name|class|    info|                interest|               score|\n",
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "|  1|建国|    1|{男, 17}|            [篮球, 足球]|{en -> 75, math -...|\n",
      "|  2|秀兰|    1|{女, 17}|      [排球, 爬山, 唱歌]|{ch -> 92, en -> 45}|\n",
      "|  3|小妹|    1|{女, 15}|[篮球, 足球, 跳舞, kpop]|{en -> 75, math -...|\n",
      "|  4|翠花|    1|{女, 14}|                  [美术]|          {ch -> 92}|\n",
      "+---+----+-----+--------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"\"\"\n",
    "id long,\n",
    "name string,\n",
    "class integer,\n",
    "info struct<sex:string, age:integer>,\n",
    "interest array<string>,\n",
    "score map<string, integer>\n",
    "\"\"\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0def9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d305408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+-------------------+\n",
      "|uid| name|height|     birth|                 dt|\n",
      "+---+-----+------+----------+-------------------+\n",
      "|  1| tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
      "|  2|tony1|  1.74|2000-01-02|2020-01-02 12:00:00|\n",
      "|  3|tony2|  1.75|2000-01-03|2020-01-03 12:00:00|\n",
      "+---+-----+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 'tony', 1.73, date(2000, 1, 1), datetime(2020, 1, 1, 12, 0)),\n",
    "    (2, 'tony1', 1.74, date(2000, 1, 2), datetime(2020, 1, 2, 12, 0)),\n",
    "    (3, 'tony2', 1.75, date(2000, 1, 3), datetime(2020, 1, 3, 12, 0))],\n",
    "    schema='uid long, name string, height double, birth date, dt timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f493d4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------------------+\n",
      "|uid|name|height|     birth|                 dt|\n",
      "+---+----+------+----------+-------------------+\n",
      "|  1|tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
      "|  2|jack|  1.78|2000-02-01|2020-01-02 12:00:00|\n",
      "|  3|mike|  1.83|2000-03-01|2020-01-03 12:01:00|\n",
      "+---+----+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'uid': [1, 2, 3],\n",
    "    'name': ['tony', 'jack', 'mike'],\n",
    "    'height': [1.73, 1.78, 1.83],\n",
    "    'birth': [date(2000, 1, 1),\n",
    "             date(2000, 2, 1),\n",
    "             date(2000, 3, 1)],\n",
    "    'dt': [datetime(2020,1,1,12,0),\n",
    "          datetime(2020,1,2,12,0),\n",
    "          datetime(2020,1,3,12,1)]\n",
    "})\n",
    "df_from_pandas = spark.createDataFrame(pandas_df)\n",
    "df_from_pandas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aaba0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+-------------------+\n",
      "|uid| name|height|     birth|                 dt|\n",
      "+---+-----+------+----------+-------------------+\n",
      "|  1| tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
      "|  2|tony1|  1.74|2000-01-02|2020-01-02 12:00:00|\n",
      "|  3|tony2|  1.75|2000-01-03|2020-01-03 12:00:00|\n",
      "+---+-----+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([\n",
    "    (1, 'tony', 1.73, date(2000, 1, 1), datetime(2020, 1, 1, 12, 0)),\n",
    "    (2, 'tony1', 1.74, date(2000, 1, 2), datetime(2020, 1, 2, 12, 0)),\n",
    "    (3, 'tony2', 1.75, date(2000, 1, 3), datetime(2020, 1, 3, 12, 0))\n",
    "])\n",
    "df_from_rdd = spark.createDataFrame(rdd, schema=['uid', 'name', 'height', 'birth', 'dt'])\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6289340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birth: date (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e172f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>uid</th><th>name</th><th>height</th><th>birth</th><th>dt</th></tr>\n",
       "<tr><td>1</td><td>tony</td><td>1.73</td><td>2000-01-01</td><td>2020-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>tony1</td><td>1.74</td><td>2000-01-02</td><td>2020-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>tony2</td><td>1.75</td><td>2000-01-03</td><td>2020-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----+------+----------+-------------------+\n",
       "|uid| name|height|     birth|                 dt|\n",
       "+---+-----+------+----------+-------------------+\n",
       "|  1| tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
       "|  2|tony1|  1.74|2000-01-02|2020-01-02 12:00:00|\n",
       "|  3|tony2|  1.75|2000-01-03|2020-01-03 12:00:00|\n",
       "+---+-----+------+----------+-------------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e67de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>uid</th><th>name</th><th>height</th><th>birth</th><th>dt</th></tr>\n",
       "<tr><td>1</td><td>tony</td><td>1.73</td><td>2000-01-01</td><td>2020-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>tony1</td><td>1.74</td><td>2000-01-02</td><td>2020-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>tony2</td><td>1.75</td><td>2000-01-03</td><td>2020-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----+------+----------+-------------------+\n",
       "|uid| name|height|     birth|                 dt|\n",
       "+---+-----+------+----------+-------------------+\n",
       "|  1| tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
       "|  2|tony1|  1.74|2000-01-02|2020-01-02 12:00:00|\n",
       "|  3|tony2|  1.75|2000-01-03|2020-01-03 12:00:00|\n",
       "+---+-----+------+----------+-------------------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2771e543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uid=1, name='tony', height=1.73, birth=datetime.date(2000, 1, 1), dt=datetime.datetime(2020, 1, 1, 12, 0)),\n",
       " Row(uid=2, name='tony1', height=1.74, birth=datetime.date(2000, 1, 2), dt=datetime.datetime(2020, 1, 2, 12, 0)),\n",
       " Row(uid=3, name='tony2', height=1.75, birth=datetime.date(2000, 1, 3), dt=datetime.datetime(2020, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f89ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| tony|\n",
      "|tony1|\n",
      "|tony2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_rdd.select(df_from_rdd.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bf37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+-------------------+----------+\n",
      "|uid| name|height|     birth|                 dt|upper_name|\n",
      "+---+-----+------+----------+-------------------+----------+\n",
      "|  1| tony|  1.73|2000-01-01|2020-01-01 12:00:00|      TONY|\n",
      "|  2|tony1|  1.74|2000-01-02|2020-01-02 12:00:00|     TONY1|\n",
      "|  3|tony2|  1.75|2000-01-03|2020-01-03 12:00:00|     TONY2|\n",
      "+---+-----+------+----------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df_from_rdd.withColumn('upper_name', upper(df_from_rdd.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac24280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------------------+\n",
      "|uid|name|height|     birth|                 dt|\n",
      "+---+----+------+----------+-------------------+\n",
      "|  1|tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
      "+---+----+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_rdd.filter(df_from_rdd.uid==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84490ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_func(height)|\n",
      "+-------------------+\n",
      "|                173|\n",
      "|                174|\n",
      "|                175|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "# 装饰器抽象讲就是为已经存在的对象添加额外的功能\n",
    "@pandas_udf('long')\n",
    "def pandas_func(col: pd.Series) -> pd.Series:\n",
    "    return col*100\n",
    "\n",
    "df_from_rdd.select(pandas_func(df_from_rdd.height)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018fc980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------------------+\n",
      "|uid|name|height|     birth|                 dt|\n",
      "+---+----+------+----------+-------------------+\n",
      "|  1|tony|  1.73|2000-01-01|2020-01-01 12:00:00|\n",
      "+---+----+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pandas_func(DataFrame):\n",
    "    for row in DataFrame:\n",
    "        yield row[row.uid == 1]\n",
    "df_from_rdd.mapInPandas(pandas_func, schema=df_from_rdd.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798de24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
