{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77fd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f493c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('heima').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c6511",
   "metadata": {},
   "source": [
    "# Transformation转换算子:返回RDD的算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93caf834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf:(T)->U 表示该算子接收一个参数的传入，传入参数类型不限，返回一个返回值，返回类型不限；\\nT和U 为泛型的代表，表示任意类型；\\nf:(A)->A 表示该算子接收一个参数传入，返回一个返回值，与传入参数类型一致；\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "f:(T)->U 表示该算子接收一个参数的传入，传入参数类型不限，返回一个返回值，返回类型不限；\n",
    "T和U 为泛型的代表，表示任意类型；\n",
    "f:(A)->A 表示该算子接收一个参数传入，返回一个返回值，与传入参数类型一致；\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f521c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map算子: f:(T)->U\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
    "rdd.map(lambda x: x * 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a68933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 3, 4, 4, 3, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap算子: 比map多了一个解除嵌套的功能   f:(T)->U\n",
    "def function_1(string):\n",
    "    str_list = string.split(\" \")\n",
    "    temp_list = []\n",
    "    for item in str_list:\n",
    "        if item == \"a\":\n",
    "            temp_list.append(1)\n",
    "        elif item == \"b\":\n",
    "            temp_list.append(2)\n",
    "        elif item == \"c\":\n",
    "            temp_list.append(3)\n",
    "        else:\n",
    "            temp_list.append(4)\n",
    "    return temp_list\n",
    "    \n",
    "rdd = sc.parallelize([\"a b c\", \"a c e\", \"e c a\"])\n",
    "rdd.flatMap(function_1).collect() # lambda x: x.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933c6289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 5), ('a', 7)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey(func)算子:f(V)->V针对KV型RDD,自动按照key分组,然后根据逻辑完成组内聚合\n",
    "# reduceByKey会先进行合并,再shuffle,再合并,因此比先groupByKey再map会效率高很多;\n",
    "# f:(V)->V\n",
    "rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)])\n",
    "rdd.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a369d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 10), ('a', 10), ('b', 20), ('b', 30), ('a', 50)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues算子:针对二元元组RDD，对其内部的Value进行map操作\n",
    "rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)])\n",
    "rdd.mapValues(lambda x: x*10).collect()  # 自动取的是value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2b6db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [('a', 1), ('a', 1)]), (1, [('b', 2), ('b', 3), ('a', 5)])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy(func)算子:f(T)->K 返回的是KV型，其中key是函数func的返回值\n",
    "# KV型中返回迭代器, value中保留的是原格式(看示例)\n",
    "rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)])\n",
    "def function_2(x):\n",
    "    if x[1] // 2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "rdd.groupBy(function_2).mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c993e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(func)算子:f(T)->bool对数据进行过滤\n",
    "def function_3(x):\n",
    "    if x % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
    "rdd.filter(function_3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2aceb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 'b', 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct算子: 对RDD数据进行去重,返回新的RDD,结果是无序的\n",
    "rdd = sc.parallelize([1, 1, 2, 2, 3, 3, 'a', 'a', 'b'])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67a6af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, ('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union算子: 将两个RDD合并为同一个RDD返回,不去重,内容类型不同依旧可以合并\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4])\n",
    "rdd2 = sc.parallelize([('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)])\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7baa46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('json', (11, None)),\n",
       " ('Jim', (12, None)),\n",
       " ('mark', (13, 'male')),\n",
       " ('jack', (12, 'male'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join、leftOuterJoin、rightOuterJoin算子:只能用于二元元组\n",
    "rdd1 = sc.parallelize([('jack', 12), ('json', 11), ('mark', 13), ('Jim', 12)])\n",
    "rdd2 = sc.parallelize([('jack', 'male'), ('mark', 'male'), ('Rose', 'female')])\n",
    "rdd1.leftOuterJoin(rdd2).collect()  # join   leftOuterJoin    rightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9418e66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection算子:求两个RDD的交集，返回新的RDD\n",
    "rdd1 = sc.parallelize([('a', 1), ('b', 2), ('c', 3)])\n",
    "rdd2 = sc.parallelize([('a', 1), 3, 4, 5, 6])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc2d797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom 算子:将RDD加上嵌套,按照分区进行嵌套\n",
    "rdd = sc.parallelize([1,2,3,4,5,6], 2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b9ccb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [2, 3]), ('a', [1, 1, 5])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey算子:对数据按照指定的规则进行分组,针对KV型数据自动按照K分组\n",
    "# 返回的value内容是迭代器,与groupBy一样\n",
    "rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 2), ('b', 3), ('a', 5)])\n",
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01a48b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('b', 3), ('a', 5), ('a', 7)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy(func, ascending, numPartition)算子:对RDD数据进行排序,排序依据基于自己指定的函数\n",
    "# 注意numPartition只能确保分区内有序,结果可能整体不有序,可设置为1即全局\n",
    "rdd = sc.parallelize([('a', 1), ('a', 7), ('b', 2), ('b', 3), ('a', 5)], 2)\n",
    "rdd.sortBy(lambda x: x[1], numPartitions=2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d4c6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('B', 1), ('c', 2), ('D', 3), ('e', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey(ascending, numPartition, keyfunc)算子:针对KV型RDD,按照Key进行排序\n",
    "# keyfunc需要显示传入,只影响排序的结果,不影响排序后原内容\n",
    "def function_4(x):\n",
    "    return x.lower()\n",
    "\n",
    "rdd = sc.parallelize([('a', 1), ('B', 1), ('c', 2), ('e', 5), ('D', 3)])\n",
    "rdd.sortByKey(ascending=True, numPartitions=1, keyfunc=function_4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2ec99",
   "metadata": {},
   "source": [
    "## Transformation下：分区操作算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70339dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 10, 20, 70, 100, 90, 20, 30]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions算子:,一次性批量处理分区数据,一次IO操作一整个分区\n",
    "# 一个分区作为一个迭代对象传出\n",
    "# 以List打包发给func,处理完以后以List返回\n",
    "def function_5(items):\n",
    "    result = []\n",
    "    for item in items:\n",
    "        result.append(item * 10)\n",
    "    return result\n",
    "\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3], 2)\n",
    "rdd.mapPartitions(function_5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac898a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachPartition算子：其特点同mapPartitions\n",
    "def function_6(items):\n",
    "    result = []\n",
    "    for item in items:\n",
    "        result.append(item * 10)\n",
    "    print(result)\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3], 2)\n",
    "rdd.foreachPartition(function_6) # [30, 10, 20, 70],[100, 90, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c17202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('hadoop', 1)], [('spark', 1)], [('python', 1), ('c++', 1)]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patitionBy(参数1：分区数, func：分区规则)算子\n",
    "# 编号从0开始到N-1结束\n",
    "def function_7(x):\n",
    "    if x == \"hadoop\":\n",
    "        return 0\n",
    "    elif x == \"spark\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "rdd = sc.parallelize([('hadoop', 1), ('spark', 1), ('python', 1), ('c++', 1)], 2)\n",
    "rdd.partitionBy(3, function_7).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92fddc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [3, 1, 2, 7], [10, 9, 2, 3]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition(N)算子：对RDD进行重新分区，实际上调用了coalesce\n",
    "# 一般情况下除了全局排序设置为1分区外,多数时候都交给spark自动设置分区数量\n",
    "# 增加分区风险大于减少分区，增加分区导致shuffle增加，性能下降\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3], 2)\n",
    "rdd.repartition(3).glom().collect()\n",
    "# coalesce(N, shuffle=True)增加分区需要，明确参数shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550357a",
   "metadata": {},
   "source": [
    "# Action动作算子:不返回RDD的算子或None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a3f0c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 3, 'b': 2})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByKey算子:统计Key出现的次数\n",
    "rdd = sc.parallelize([('a', 1), ('a', 7), ('b', 2), ('b', 3), ('a', 5)], 2)\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1750f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect算子:将RDD各个分区的数据都拉取到Driver,在使用之前要判断内存占用可能\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "179e7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# reduce(func)算子:func(T, T)->T对RDD数据按照传入的逻辑进行聚合\n",
    "rdd = sc.parallelize(range(1, 10))\n",
    "print(rdd.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a09f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold(init_value, func)算子:接收一个传入逻辑,进行带有初始值的聚合\n",
    "# 会作用在每个分区,会额外带上每个分区的初始值聚合\n",
    "rdd = sc.parallelize(range(1, 10), 3)\n",
    "rdd.fold(10, lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "914665bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first算子\n",
    "rdd = sc.parallelize([3, 1, 2])\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2f60501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take算子：返回RDD的前N个元素\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13d05259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 7]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top算子：对RDD数据集进行降序排序,取前N个返回\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58a40d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count算子：计算RDD有多少条数据,返回一个数字\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "015ec9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 9, 10, 7, 3, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeSample(参数1:可放回,参数2:个数,参数3:随机数种子)算子:随机抽样RDD的数据\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "028ae7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 7, 3, 3, 2, 2, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeOrdered(参数1:个数,func):功能对RDD进行排序取前N个，比Top好的点在于，可升序可降序\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.takeOrdered(10, lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5405a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach算子:对RDD的每一个元素执行提供的操作逻辑,没有返回值;\n",
    "# 经由excutor直接输出，与collect等算子不一样，不与Driver汇报\n",
    "# “不生成新的RDD”，可以进行数据库插入 (saveAsTextFile一样)\n",
    "\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3])\n",
    "rdd.foreach(lambda x: print(x * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59375eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAsTextFile算子：将RDD的数据写入文本文件中\n",
    "# 支持本地写出，hdfs等文件系统\n",
    "rdd = sc.parallelize([3, 1, 2, 7, 10, 9, 2, 3], 2)\n",
    "rdd.saveAsTextFile('./output2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
